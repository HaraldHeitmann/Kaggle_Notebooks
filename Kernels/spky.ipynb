{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:43.940000Z",
     "start_time": "2017-11-03T15:48:43.901000Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from IPython.display import display\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LogisticRegression as LoRe\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:44.560000Z",
     "start_time": "2017-11-03T15:48:44.549000Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:45.010000Z",
     "start_time": "2017-11-03T15:48:44.915000Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Spooky/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:45.537000Z",
     "start_time": "2017-11-03T15:48:45.517000Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:46.393000Z",
     "start_time": "2017-11-03T15:48:46.066000Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: ' '.join(tokenizer.tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:47.064000Z",
     "start_time": "2017-11-03T15:48:46.854000Z"
    }
   },
   "outputs": [],
   "source": [
    "df['length']=df.text.apply(lambda x:len(x.split(' ')))\n",
    "df['unique_l']=df.text.apply(lambda x:len(set(x.split(' '))))\n",
    "df['diversity'] = df.unique_l/df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:47.974000Z",
     "start_time": "2017-11-03T15:48:47.604000Z"
    }
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots()\n",
    "for auth in df.author.unique():\n",
    "    df.diversity[df.author==auth].hist(alpha=0.5,label=auth,ax=ax)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:48.951000Z",
     "start_time": "2017-11-03T15:48:48.628000Z"
    }
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots()\n",
    "for auth in df.author.unique():\n",
    "    df.length[df.author==auth].hist(alpha=0.5,label=auth,ax=ax)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:50.117000Z",
     "start_time": "2017-11-03T15:48:49.781000Z"
    }
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots()\n",
    "for auth in df.author.unique():\n",
    "    df.unique_l[df.author==auth].hist(alpha=0.5,label=auth,ax=ax)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:52.023000Z",
     "start_time": "2017-11-03T15:48:52.005000Z"
    }
   },
   "outputs": [],
   "source": [
    "def bagging(text):\n",
    "    dictio={}\n",
    "    for word in text.split(' '):\n",
    "        if word in dictio.keys():\n",
    "            dictio[word]+=1\n",
    "        else:\n",
    "            dictio[word]=1\n",
    "    return dictio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:53.628000Z",
     "start_time": "2017-11-03T15:48:53.038000Z"
    }
   },
   "outputs": [],
   "source": [
    "df['bagged']=df.text.apply(bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-03T15:48:58.045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_auth={}\n",
    "for auth in df.author.unique():\n",
    "    df_temp=df[df.author==auth]\n",
    "    result={}\n",
    "    for i in range(len(df_temp.bagged)):\n",
    "        x=df_temp.bagged.iloc[i]\n",
    "        result={ k: x.get(k, 0) + result.get(k, 0) for k in set(x) | set(result) }\n",
    "    per_auth[auth]=result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-03T15:49:08.378Z"
    }
   },
   "outputs": [],
   "source": [
    "# for word in set(per_auth['EAP'])|set(per_auth['MWS'])|set(per_auth['HPL']):\n",
    "#     for auth in df.author.unique():\n",
    "#         try:\n",
    "#             print '{}: {} has count: {}'.format(auth,word,per_auth[auth][word])\n",
    "#         except KeyError:\n",
    "#             print '{}: {} has count: {}'.format(auth,word,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-03T15:49:08.589Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in set(per_auth['EAP'])|set(per_auth['MWS'])|set(per_auth['HPL']):\n",
    "    for auth in df.author.unique():\n",
    "        if word in per_auth[auth]:\n",
    "            pass\n",
    "        else:\n",
    "            per_auth[auth][word]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-03T15:49:08.772Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for word in per_auth['EAP'].keys():\n",
    "    author = ''\n",
    "    maxi = 0\n",
    "    suma=0\n",
    "    for auth in df.author.unique():\n",
    "        suma+=per_auth[auth][word]\n",
    "        if per_auth[auth][word]>maxi: # colisions on draws!\n",
    "            author=auth\n",
    "            maxi=per_auth[auth][word]\n",
    "    scores[word]=(author,maxi/suma,maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T13:59:42.083000Z",
     "start_time": "2017-11-03T13:59:42.077000Z"
    }
   },
   "outputs": [],
   "source": [
    "### scores is my trained model... basic basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T13:59:55.580000Z",
     "start_time": "2017-11-03T13:59:54.899000Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    N = np.random.randint(len(df))\n",
    "    text = df.text.iloc[N]\n",
    "    target = df.author.iloc[N] \n",
    "    print target,': ',text\n",
    "    score = {'HPL':0,'EAP':0,'MWS':0}\n",
    "    for word in text.split(' '):\n",
    "        if word not in scores.keys():\n",
    "            pass\n",
    "        else:\n",
    "            Key,Sc,L=scores[word]\n",
    "            score[Key]+=Sc\n",
    "    tot=0\n",
    "    for k in score.keys():\n",
    "        tot+=score[k]\n",
    "    for k in score.keys():\n",
    "        score[k]=score[k]/tot\n",
    "#     print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-30T14:18:57.995000Z",
     "start_time": "2017-10-30T14:18:57.987000Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T14:22:02.217000Z",
     "start_time": "2017-11-03T14:07:55.222000Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for X in df.iterrows():\n",
    "    prediction={}\n",
    "    txt=X[1].text\n",
    "    ID=X[1].id\n",
    "    score = {'HPL':0,'EAP':0,'MWS':0}\n",
    "    for word in txt.split(' '):\n",
    "        if word not in scores.keys():\n",
    "            pass\n",
    "        else:\n",
    "            Key,Sc,L=scores[word]\n",
    "            score[Key]+=Sc\n",
    "    tot=0\n",
    "    for k in score.keys():\n",
    "        tot+=score[k]\n",
    "    for k in score.keys():\n",
    "        prediction[k]=score[k]/tot\n",
    "    prediction['id']=ID\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T14:22:07.772000Z",
     "start_time": "2017-11-03T14:22:07.708000Z"
    }
   },
   "outputs": [],
   "source": [
    "subs=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T14:22:09.195000Z",
     "start_time": "2017-11-03T14:22:09.165000Z"
    }
   },
   "outputs": [],
   "source": [
    "subs = subs[['id','EAP','HPL','MWS']] \n",
    "subs.columns=['id','EAP_feat','HPL_feat','MWS_feat']\n",
    "subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T14:22:19.561000Z",
     "start_time": "2017-11-03T14:22:19.534000Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.merge(df,subs,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T14:22:33.100000Z",
     "start_time": "2017-11-03T14:22:33.060000Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:46:05.391000Z",
     "start_time": "2017-11-03T15:46:05.311000Z"
    }
   },
   "outputs": [],
   "source": [
    "x=np.random.binomial(1,0.8,len(new_df))\n",
    "lblbin = LabelBinarizer()\n",
    "lblbin.fit(new_df.author.unique())\n",
    "trans=lblbin.transform(new_df.author.values)\n",
    "new_df['EAP_bin']= trans[:,0]\n",
    "new_df['HPL_bin']= trans[:,1]\n",
    "new_df['MWS_bin']= trans[:,2]\n",
    "# data= new_df[['diversity','EAP_feat','HPL_feat','MWS_feat','EAP_bin','HPL_bin','MWS_bin']].values\n",
    "# train_d=data[x==1][:,:-3]\n",
    "# test_d=data[x==0][:,-3:]\n",
    "# train_l=data[x==1][:,:-3]\n",
    "# test_l=data[x==0][:,-3:]\n",
    "data= new_df[['diversity','EAP_feat','HPL_feat','MWS_feat','author']].values\n",
    "train_d=data[x==1][:,:-1]\n",
    "train_l=data[x==1][:,-1]\n",
    "test_d=data[x==0][:,:-1]\n",
    "test_l=data[x==0][:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:46:05.946000Z",
     "start_time": "2017-11-03T15:46:05.519000Z"
    }
   },
   "outputs": [],
   "source": [
    "lore = LoRe(solver='lbfgs')\n",
    "lore.fit(train_d,train_l)\n",
    "pred_p = lore.predict_proba(test_d)\n",
    "pred = lore.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:46:05.974000Z",
     "start_time": "2017-11-03T15:46:05.948000Z"
    }
   },
   "outputs": [],
   "source": [
    "log_loss(test_l,pred_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:46:07.352000Z",
     "start_time": "2017-11-03T15:46:07.151000Z"
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "lore = LoRe()\n",
    "lore.fit(X=new_df[['diversity','EAP_feat','HPL_feat','MWS_feat']].values,y=new_df.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:46:07.663000Z",
     "start_time": "2017-11-03T15:46:07.518000Z"
    }
   },
   "outputs": [],
   "source": [
    "log_loss(new_df.author.values,lore.predict_proba(new_df[['diversity','EAP_feat','HPL_feat','MWS_feat']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:47:25.657000Z",
     "start_time": "2017-11-03T15:47:25.367000Z"
    }
   },
   "outputs": [],
   "source": [
    "smp=pd.read_csv('../data/Spooky/sample_submission.csv')\n",
    "display(smp.head())\n",
    "test=pd.read_csv('../data/Spooky/test.csv')\n",
    "test.text = test.text.apply(lambda x: ' '.join(tokenizer.tokenize(x.lower())))\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:33:25.726000Z",
     "start_time": "2017-11-03T15:26:53.625000Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for X in test.iterrows():\n",
    "    prediction={}\n",
    "    txt=X[1].text\n",
    "    ID=X[1].id\n",
    "    score = {'HPL':0,'EAP':0,'MWS':0}\n",
    "    for word in txt.split(' '):\n",
    "        if word not in scores.keys():\n",
    "            pass\n",
    "        else:\n",
    "            Key,Sc,L=scores[word]\n",
    "            score[Key]+=Sc\n",
    "    tot=0\n",
    "    for k in score.keys():\n",
    "        tot+=score[k]\n",
    "    for k in score.keys():\n",
    "        prediction[k]=score[k]/tot\n",
    "    prediction['id']=ID\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:47:30.618000Z",
     "start_time": "2017-11-03T15:47:30.562000Z"
    }
   },
   "outputs": [],
   "source": [
    "test_feat = pd.DataFrame(predictions)\n",
    "test_feat = test_feat[['id','EAP','HPL','MWS']] \n",
    "test_feat.columns=['id','EAP_feat','HPL_feat','MWS_feat']\n",
    "test_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:47:46.292000Z",
     "start_time": "2017-11-03T15:47:46.170000Z"
    }
   },
   "outputs": [],
   "source": [
    "test['length']=test.text.apply(lambda x:len(x.split(' ')))\n",
    "test['unique_l']=test.text.apply(lambda x:len(set(x.split(' '))))\n",
    "test['diversity'] = test.unique_l/df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:47:47.430000Z",
     "start_time": "2017-11-03T15:47:47.380000Z"
    }
   },
   "outputs": [],
   "source": [
    "Test=pd.merge(test,test_feat, on='id')\n",
    "Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:48:25.538000Z",
     "start_time": "2017-11-03T15:48:25.516000Z"
    }
   },
   "outputs": [],
   "source": [
    "Test.diversity.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:41:47.018000Z",
     "start_time": "2017-11-03T15:41:46.998000Z"
    }
   },
   "outputs": [],
   "source": [
    "data= Test[['diversity','EAP_feat','HPL_feat','MWS_feat']].values\n",
    "pred_prob = lore.predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:41:59.729000Z",
     "start_time": "2017-11-03T15:41:59.661000Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submi= pd.DataFrame(pred_prob)\n",
    "submi['id'] = Test.id\n",
    "submi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:42:32.284000Z",
     "start_time": "2017-11-03T15:42:32.221000Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submi.columns=['EAP','HPL','MWS','id']\n",
    "submi\n",
    "submi=submi[['id','EAP','HPL','MWS']] \n",
    "submi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:42:49.420000Z",
     "start_time": "2017-11-03T15:42:49.362000Z"
    }
   },
   "outputs": [],
   "source": [
    "submi.to_csv('../data/Spooky/submission_3.csv ',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T15:42:50.177000Z",
     "start_time": "2017-11-03T15:42:50.101000Z"
    }
   },
   "outputs": [],
   "source": [
    "submi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-03T15:49:43.673Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
